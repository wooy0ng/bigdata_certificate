{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 작업형 제1유형"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lst = [2, 3, 3.2, 5, 7.5, 10, 11.8, 12, 23, 25, 31.5, 34]\n",
    "q1, q3 = pd.Series(lst).quantile([0.25, 0.75])\n",
    "\n",
    "print(int(abs(q1 - q3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../yemoonsaBigdata/datasets/Part3/402_facebook.csv')\n",
    "\n",
    "# print(df.columns) # num_loves, num_wows\n",
    "\n",
    "df['tmp'] = (df['num_loves'] + df['num_wows']) / df['num_reactions']\n",
    "\n",
    "tmp = df.loc[(df['tmp'] > 0.4) & (df['tmp'] < 0.5), :]\n",
    "print(len(tmp[tmp['status_type'] == 'video']))\n",
    "\n",
    "# print(df['num_loves_wows'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('../yemoonsaBigdata/datasets/Part3/403_netflix.csv')\n",
    "new_df = df.loc[df['date_added'].str.contains(\"January\") & df['date_added'].str.contains(\"2018\"), :]\n",
    "\n",
    "print(len(new_df.loc[new_df['country'] == 'United Kingdom']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "### 작업형 제2유형\n",
    "\n",
    "- 분류 문제 (multi label)\n",
    "- macro f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5082117976695973\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X_train = pd.read_csv('../yemoonsaBigdata/datasets/Part3/404_x_train.csv')\n",
    "y_train = pd.read_csv('../yemoonsaBigdata/datasets/Part3/404_y_train.csv')\n",
    "\n",
    "X_test = pd.read_csv('../yemoonsaBigdata/datasets/Part3/404_x_test.csv')\n",
    "\n",
    "unused_cols = ['ID']\n",
    "num_cols = ['Age', 'Work_Experience', 'Family_Size']\n",
    "cat_cols = ['Gender', 'Ever_Married', 'Graduated', 'Profession', 'Spending_Score']\n",
    "y_cols = ['Segmentation']\n",
    "\n",
    "# 범주형 데이터 전처리\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = pd.concat([X_train[cat_cols], X_test[cat_cols]])\n",
    "\n",
    "for col in cat_cols:\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(X[col])\n",
    "    \n",
    "    X_train[col] = label_encoder.transform(X_train[col])\n",
    "    X_test[col] = label_encoder.transform(X_test[col])\n",
    "    \n",
    "# 정답 데이터 전처리\n",
    "for col in y_cols:\n",
    "    y_label_encoder = LabelEncoder()\n",
    "    y_label_encoder.fit(y_train[col])\n",
    "    \n",
    "    y_train[col] = y_label_encoder.transform(y_train[col])\n",
    "\n",
    "\n",
    "# 학습, 검증 데이터셋 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train[num_cols+cat_cols], y_train[y_cols], test_size=0.3, stratify=y_train[y_cols])\n",
    "\n",
    "\n",
    "# 수치형 데이터 전처리\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_tr[num_cols])\n",
    "\n",
    "X_tr[num_cols] = scaler.transform(X_tr[num_cols])\n",
    "X_val[num_cols] = scaler.transform(X_val[num_cols])\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "\n",
    "# 학습\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "'''\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_tr, y_tr.values.squeeze())\n",
    "\n",
    "pred_val = model_rf.predict(X_val)\n",
    "print(f1_score(y_val, pred_val, average='macro'))\n",
    "\n",
    "model_xgb = XGBClassifier()\n",
    "model_xgb.fit(X_tr, y_tr.values.squeeze())\n",
    "\n",
    "pred_val = model_xgb.predict(X_val)\n",
    "print(f1_score(y_val, pred_val, average='macro'))\n",
    "'''    \n",
    "\n",
    "model_xgb1 = XGBClassifier(n_estimators=50, max_depth=5)\n",
    "model_xgb1.fit(X_tr, y_tr.values.squeeze())\n",
    "\n",
    "pred_val = model_xgb1.predict(X_val)\n",
    "print(f1_score(y_val, pred_val, average='macro'))\n",
    "\n",
    "\n",
    "# 테스트\n",
    "pred = model_xgb1.predict(X_test[num_cols+cat_cols])\n",
    "pred = y_label_encoder.inverse_transform(pred)\n",
    "\n",
    "result = pd.DataFrame({\n",
    "    'ID': X_test['ID'],\n",
    "    'Segmentation': pred\n",
    "})\n",
    "\n",
    "result.to_csv('../yemoonsaBigdata/res/2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
